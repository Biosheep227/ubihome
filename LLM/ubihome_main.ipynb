{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import ollama\n",
    "import chromadb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# Pipeline functions\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv('simulated_data/reading.csv')\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    def interpret_brightness(brightness_level):\n",
    "        if 0 <= brightness_level <= 100:\n",
    "            return \"dark\"\n",
    "        elif 100 < brightness_level <= 150:\n",
    "            return \"dim\"\n",
    "        elif 150 < brightness_level <= 500:\n",
    "            return \"normal lighting\"\n",
    "        elif 500 < brightness_level <= 1000:\n",
    "            return \"very bright\"\n",
    "        else:\n",
    "            return \"dark\"\n",
    "\n",
    "    def generate_description(row):\n",
    "        brightness_description = interpret_brightness(row['BrightnessLevel'])\n",
    "        # return f\"{row['UserActivity']} in the {row['Location']} when it is {brightness_description} at {row['Timestamp']}.\"\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        description = f\"{row['UserActivity']} on the {row['Location']} at {row['Timestamp']} when it is {brightness_description}.\"\n",
    "        return description\n",
    "\n",
    "    df['Description'] = df.apply(generate_description, axis=1)\n",
    "\n",
    "    # only use the lastest state\n",
    "    df_latest = df.iloc[-1:]\n",
    "    df_latest = df_latest['Description']\n",
    "    df_latest = df_latest.to_string(index=False)\n",
    "    # print(df_latest)\n",
    "\n",
    "    # use last 10 states\n",
    "    df_history = df.iloc[-10:]\n",
    "    df_history = df_history['Description']\n",
    "    df_history = df_history.to_string(index=False)\n",
    "    # print(df_history)\n",
    "\n",
    "    return df_latest, df_history\n",
    "\n",
    "\n",
    "    \n",
    "def LLM2(user, knowledge_base, df_latest, collection):\n",
    "\n",
    "    # user_query = f\"\"\"\n",
    "    # There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    # Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    # \"\"\" \n",
    "\n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment. \n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. light5 is the main light in the dining room next to living room closely.\n",
    "    tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    \"\"\" \n",
    "\n",
    "    print(\"User Query:\")\n",
    "    print(user_query)\n",
    "\n",
    "    query_response = ollama.embeddings(\n",
    "        prompt=user_query,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_response[\"embedding\"]],\n",
    "        n_results=5  # Retrieve top 3 relevant documents\n",
    "    )\n",
    "\n",
    "    retrieved_documents = results['documents'][0]  \n",
    "    data = ' '.join(retrieved_documents)\n",
    "    print(\"Retrieved Data:\")\n",
    "    print(data)\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Based on the knowledge base entries:\n",
    "    {data}\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def LLM1(df_latest, user_feedback, actions, df_history):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_update = f\"\"\"\n",
    "    You are a smart home assistant responsible for deciding if to update the lighting knowledge based on user manually changes to the light state and recent user behaviors. \n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, there are some trasition phase, so decide if to update the knowledge base based on the user feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    query_response = ollama.embeddings(\n",
    "        prompt=user_update,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_response[\"embedding\"]],\n",
    "        n_results=8  # Retrieve top 10 relevant documents\n",
    "    )\n",
    "\n",
    "    retrieved_documents = results['documents'][0]  \n",
    "    data = ' '.join(retrieved_documents)\n",
    "    print(\"Retrieved Data:\")\n",
    "    print(data)\n",
    "\n",
    "    final_prompt_update = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Based on the retrieved current knowledge base entries:\n",
    "    {data}\n",
    "\n",
    "    Consider current situation:\n",
    "    {user_update}\n",
    "\n",
    "    Decide if to update the knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"Update\": \"yes/no\"  \n",
    "    }}\n",
    "\n",
    "    Provide the updated knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    ## LLM1\n",
    "    output = ollama.generate(\n",
    "    model=\"qwen2.5:14b-instruct\",\n",
    "    prompt=final_prompt_update\n",
    "    )\n",
    "\n",
    "    knowledge_base_text = output['response']\n",
    "    # print(knowledge_base_text)\n",
    "    return knowledge_base_text\n",
    "\n",
    "\n",
    "\n",
    "def update_knowledge_base(knowledge_base, knowledge_base_update):\n",
    "    code_block_pattern = r'```json\\s*([\\s\\S]*?)```'\n",
    "    json_blocks = re.findall(code_block_pattern, knowledge_base_update)\n",
    "\n",
    "    # Initialize variables to store the extracted data\n",
    "    update_decision = None\n",
    "    updated_kb = None\n",
    "\n",
    "    for idx, json_str in enumerate(json_blocks):\n",
    "        print(f\"\\nExtracted JSON String {idx+1}:\")\n",
    "        print(json_str)\n",
    "\n",
    "        # Clean up the JSON string\n",
    "        json_str = json_str.strip()\n",
    "        json_str = re.sub(r',\\s*}', '}', json_str)  # Remove trailing commas before }\n",
    "        json_str = re.sub(r',\\s*\\]', ']', json_str)  # Remove trailing commas before ]\n",
    "\n",
    "        # Parse the JSON string\n",
    "        try:\n",
    "            json_data = json.loads(json_str)\n",
    "\n",
    "            # Determine the content of the JSON block\n",
    "            if \"Update\" in json_data:\n",
    "                update_decision = json_data.get(\"Update\")\n",
    "                print(f\"\\nParsed Update Decision: {update_decision}\")\n",
    "            else:\n",
    "                # Assuming the other JSON block is the updated knowledge base\n",
    "                updated_kb = json_data\n",
    "                print(\"\\nParsed Updated Knowledge Base:\")\n",
    "                print(json.dumps(updated_kb, indent=2))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nFailed to parse JSON in block {idx+1}: {e}\")\n",
    "\n",
    "    # Process the extracted data\n",
    "    if update_decision is not None:\n",
    "        print(f\"\\nUpdate Decision: {update_decision}\")\n",
    "        if update_decision.lower() == \"yes\":\n",
    "            if updated_kb is not None:\n",
    "                # Update the knowledge base\n",
    "                knowledge_base_update = updated_kb\n",
    "                print(\"\\nKnowledge base has been updated.\")\n",
    "            else:\n",
    "                print(\"\\nNo updated knowledge base provided.\")\n",
    "        else:\n",
    "            print(\"\\nKnowledge base remains unchanged.\")\n",
    "    else:\n",
    "        print(\"\\nThe 'Update' decision was not found in the LLM's output.\")\n",
    "\n",
    "    knowledge_base = knowledge_base+ str(knowledge_base_update)\n",
    "    return knowledge_base\n",
    "\n",
    "\n",
    "def get_user_confidence():\n",
    "    confidence_input = input(\"Please rate your confidence in the system on a scale from 0 to 1: \")\n",
    "    try:\n",
    "        confidence = float(confidence_input)\n",
    "        if 0 <= confidence <= 1:\n",
    "            return confidence\n",
    "        else:\n",
    "            print(\"Invalid input. Confidence should be between 0 and 1.\")\n",
    "            return get_user_confidence()\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number between 0 and 1.\")\n",
    "        return get_user_confidence()\n",
    "    \n",
    "\n",
    "knowledge_base = \"\"\"\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_json = \"\"\"\n",
    "{\n",
    "  \"Layout of lights in the living room\": [\n",
    "    {\n",
    "      \"light_id\": \"light1\",\n",
    "      \"description\": \"Main light with 3 color temperatures: warm, neutral, and cool.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light2\",\n",
    "      \"description\": \"Dimming light above the TV.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light3\",\n",
    "      \"description\": \"Lamp on the table.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light4\",\n",
    "      \"description\": \"Curtain on the window.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light5\",\n",
    "      \"description\": \"Main light in the dining room next to the living room.\"\n",
    "    }\n",
    "    \n",
    "  ],\n",
    "  \"users\": [\n",
    "    {\n",
    "      \"user_id\": \"user1\",\n",
    "      \"name\": \"Richard\",\n",
    "      \"preferences\": [\n",
    "        {\n",
    "          \"activity\": \"watching TV\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"reading\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"cooking\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"sitting\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"lying\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"typing\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"housekeeping\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"exercising\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"video gaming\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"eating\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"grooming\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user2\",\n",
    "      \"name\": \"Di\",\n",
    "      \"preferences\": [\n",
    "        {\n",
    "          \"activity\": \"watching TV\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"reading\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"cooking\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"sitting\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"lying\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"typing\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"housekeeping\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"exercising\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"video gaming\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"eating\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"grooming\",\n",
    "            \"preferred_lighting\": \"dimly lit\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user3\",\n",
    "      \"name\": \"Mingyi\",\n",
    "      \"preferences\": [\n",
    "        {\n",
    "          \"activity\": \"watching TV\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"reading\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"cooking\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"sitting\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"lying\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"typing\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"housekeeping\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"exercising\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"video gaming\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"eating\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"grooming\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# Pipeline Main\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = knowledge_base.split('\\n\\n')\n",
    "documents = [' '.join(doc.split()) for doc in documents]\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "try:\n",
    "    collection = client.get_collection(\"docs\")\n",
    "    print(\"Collection already exists.\")\n",
    "except Exception as e:\n",
    "    # If the collection does not exist, create it\n",
    "    collection = client.create_collection(name=\"docs\")\n",
    "    print(\"Collection created.\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "    response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "    embedding = response[\"embedding\"]\n",
    "    collection.add(\n",
    "        ids=[str(i)],\n",
    "        embeddings=[embedding],\n",
    "        documents=[d]\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "\n",
    "    user = \"Richard\"\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = LLM2(user, knowledge_base, df_latest, collection)\n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"on\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            if user_feedback:\n",
    "                user_condidence = get_user_confidence()\n",
    "                # if user_condidence < 0.5:\n",
    "                #     # use RL agent to update the knowledge base\n",
    "                #     print(\"RL agent to update the knowledge base\")\n",
    "                #     knowledge_base = rl_agent_update(knowledge_base, user_feedback, actions, df_history)\n",
    "\n",
    "                # else:\n",
    "                #     knowledge_base_update = LLM1(user_feedback, actions, df_history)\n",
    "                #     knowledge_base = update_knowledge_base(knowledge_base, knowledge_base_update)\n",
    "                #     print(\"\\nUpdated Knowledge Base:\")\n",
    "                #     print(knowledge_base)\n",
    "                print(user_feedback)\n",
    "                # knowledge_base_update = LLM1(user_feedback, actions, df_history, collection)\n",
    "                knowledge_base_update = LLM1(\n",
    "                    df_latest=df_latest,\n",
    "                    user_feedback=user_feedback,\n",
    "                    actions=actions,\n",
    "                    df_history=df_history)\n",
    "                \n",
    "                knowledge_base = update_knowledge_base(knowledge_base, knowledge_base_update)\n",
    "                print(\"\\nUpdated Knowledge Base:\")\n",
    "                print(knowledge_base)\n",
    "\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# 0-Shot\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "reading on the sofa at 11:04:00 when it is very bright.\n",
      "User Feedback:\n",
      "None\n",
      "Assistant's Response:\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Suggested Action:\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "User Feedback:\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[43mmain_zeroshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 112\u001b[0m, in \u001b[0;36mmain_zeroshot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     user_feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m actions_feedback \u001b[38;5;241m=\u001b[39m \u001b[43mzero_shot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_latest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_latest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_feedback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_feedback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUpdated Action:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(actions_feedback)\n",
      "Cell \u001b[0;32mIn[4], line 64\u001b[0m, in \u001b[0;36mzero_shot\u001b[0;34m(user, df_latest, user_feedback, df_history, actions)\u001b[0m\n\u001b[1;32m     35\u001b[0m user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is the user who is living in this apartment. \u001b[39m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124mConsidering recent user behaviors, decide if to update the lights base based on the user feedback.\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     46\u001b[0m final_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124mYou are a smart home assistant.\u001b[39m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 64\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# model=\"qwen2.5:7b\",\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen2.5:14b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_prompt\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m actions \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/ollama/_client.py:163\u001b[0m, in \u001b[0;36mClient.generate\u001b[0;34m(self, model, prompt, suffix, system, template, context, stream, raw, format, images, options, keep_alive)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust provide a model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemplate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/ollama/_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/ollama/_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http_proxy.py:207\u001b[0m, in \u001b[0;36mForwardHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m url \u001b[38;5;241m=\u001b[39m URL(\n\u001b[1;32m    195\u001b[0m     scheme\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_origin\u001b[38;5;241m.\u001b[39mscheme,\n\u001b[1;32m    196\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_origin\u001b[38;5;241m.\u001b[39mhost,\n\u001b[1;32m    197\u001b[0m     port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[1;32m    198\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbytes\u001b[39m(request\u001b[38;5;241m.\u001b[39murl),\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m proxy_request \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    201\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    202\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    206\u001b[0m )\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def zero_shot(user, df_latest, user_feedback, df_history, actions):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment. \n",
    "\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, decide if to update the lights base based on the user feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "def main_zeroshot():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "    user = \"Richard\"\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = zero_shot(\n",
    "                user = user,\n",
    "                df_latest = df_latest,\n",
    "                user_feedback = None,\n",
    "                df_history = None,\n",
    "                actions = None)\n",
    "            \n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "        \n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            actions_feedback = zero_shot(\n",
    "                user=user,\n",
    "                df_latest=df_latest,\n",
    "                user_feedback=user_feedback,\n",
    "                df_history=df_history,\n",
    "                actions=actions)\n",
    "            \n",
    "            print(\"\\nUpdated Action:\")\n",
    "            print(actions_feedback)\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_zeroshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# Chain of Thought\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "reading on the sofa at 11:04:00 when it is very bright.\n",
      "User Feedback:\n",
      "None\n",
      "Assistant's Response:\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Suggested Action:\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "User Feedback:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}\n",
      "Assistant's Response:\n",
      "Based on Richard's recent behaviors and the current situation, it seems that he prefers a darker environment while reading in the living room during very bright conditions. Given that all lights were turned off except for light3 (the table lamp) which was briefly switched back to off by him, it indicates a desire for minimal lighting.\n",
      "\n",
      "Here are the actions considering his preferences and recent behaviors:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "**Reasoning Steps:**\n",
      "1. **Light1 (Main Living Room Light):** Turned off as its the brightest and was previously turned off by Richard.\n",
      "2. **Light2 (Dimming Light Above TV):** Turned off to minimize bright light exposure while reading.\n",
      "3. **Light3 (Table Lamp):** Turned on since a very low-level, focused light might be beneficial for reading during bright conditions without causing glare.\n",
      "4. **Light4 (Curtains):** Open curtains as requested by Richard earlier, allowing natural light but with table lamp providing necessary artificial light.\n",
      "5. **Light5 (Dining Room Light):** Turned off since its in the adjacent room and not affecting the reading environment directly.\n",
      "\n",
      "This configuration should cater to Richard's preference for minimal lighting while maintaining a comfortable setting for reading during bright times of day.\n",
      "\n",
      "Updated Action:\n",
      "Based on Richard's recent behaviors and the current situation, it seems that he prefers a darker environment while reading in the living room during very bright conditions. Given that all lights were turned off except for light3 (the table lamp) which was briefly switched back to off by him, it indicates a desire for minimal lighting.\n",
      "\n",
      "Here are the actions considering his preferences and recent behaviors:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "**Reasoning Steps:**\n",
      "1. **Light1 (Main Living Room Light):** Turned off as its the brightest and was previously turned off by Richard.\n",
      "2. **Light2 (Dimming Light Above TV):** Turned off to minimize bright light exposure while reading.\n",
      "3. **Light3 (Table Lamp):** Turned on since a very low-level, focused light might be beneficial for reading during bright conditions without causing glare.\n",
      "4. **Light4 (Curtains):** Open curtains as requested by Richard earlier, allowing natural light but with table lamp providing necessary artificial light.\n",
      "5. **Light5 (Dining Room Light):** Turned off since its in the adjacent room and not affecting the reading environment directly.\n",
      "\n",
      "This configuration should cater to Richard's preference for minimal lighting while maintaining a comfortable setting for reading during bright times of day.\n",
      "description not changed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[43mmain_cot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 128\u001b[0m, in \u001b[0;36mmain_cot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription not changed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def cot(user, df_latest, user_feedback, df_history, actions):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment.\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, decide if to update the lights base based on the user feedback.\n",
    "\n",
    "    Provide your reasoning step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "def main_cot():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "    user =  \"Richard\"\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = cot(\n",
    "                user = user,\n",
    "                df_latest = df_latest,\n",
    "                user_feedback = None,\n",
    "                df_history = None,\n",
    "                actions = None)\n",
    "            \n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "        \n",
    "\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            actions_feedback = cot(\n",
    "                user=user,\n",
    "                df_latest=df_latest,\n",
    "                user_feedback=user_feedback,\n",
    "                df_history=df_history,\n",
    "                actions=actions)\n",
    "            \n",
    "            print(\"\\nUpdated Action:\")\n",
    "            print(actions_feedback)\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_cot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# ReACT\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "reading on the sofa at 11:04:00 when it is very bright.\n",
      "User Feedback:\n",
      "None\n",
      "Assistant's Response:\n",
      "Since Richard is reading on the sofa at 11:04 PM and it is very bright, we need to consider a comfortable lighting environment for reading while ensuring that other areas are not unnecessarily illuminated. Given the time of day (evening), the lights should be adjusted for a calming and focused ambiance without overwhelming brightness.\n",
      "\n",
      "**Reasoning Steps:**\n",
      "\n",
      "1. **Light1 (Main Light in Living Room):** Since it is very bright, but Richard is reading, we can assume that direct sunlight or other external light sources are still providing ample illumination. In this case, switching off the main light would be appropriate to avoid glare and eye strain.\n",
      "   \n",
      "2. **Light2 (Dimming Light above TV):** This light is likely not necessary for reading purposes on the sofa, as it's typically used more for ambient lighting or watching TV.\n",
      "\n",
      "3. **Light3 (Lamp on Table):** Considering Richard is reading, this would be the most appropriate light to turn on and adjust to a comfortable brightness level suitable for reading. If the room is very bright due to external lights, reducing the brightness of Light1 might make it unnecessary, but keeping Light3 on will ensure good visibility.\n",
      "\n",
      "4. **Light4 (Curtain):** Since its still quite bright at 11:04 PM and Richard is reading indoors, opening or closing the curtains won't have a significant impact if there's no direct sunlight affecting the brightness inside the room. However, to reduce glare from external lights, closing Light4 might be beneficial.\n",
      "\n",
      "5. **Light5 (Main Light in Dining Room):** Since this light isnt directly related to Richards current activity of reading on the sofa and considering it is already very bright, theres no need to turn this light on or off unless it impacts the brightness level where he's sitting.\n",
      "\n",
      "Based on these considerations:\n",
      "\n",
      "- Turn off Light1.\n",
      "- Keep Light3 (adjust its dimmer if necessary).\n",
      "- Consider closing Light4 (if glare is an issue).\n",
      "\n",
      "**Action: Adjust based on reasoning.**\n",
      "\n",
      "**Final Light Control Actions:**\n",
      "```json\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04 PM when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Suggested Action:\n",
      "Since Richard is reading on the sofa at 11:04 PM and it is very bright, we need to consider a comfortable lighting environment for reading while ensuring that other areas are not unnecessarily illuminated. Given the time of day (evening), the lights should be adjusted for a calming and focused ambiance without overwhelming brightness.\n",
      "\n",
      "**Reasoning Steps:**\n",
      "\n",
      "1. **Light1 (Main Light in Living Room):** Since it is very bright, but Richard is reading, we can assume that direct sunlight or other external light sources are still providing ample illumination. In this case, switching off the main light would be appropriate to avoid glare and eye strain.\n",
      "   \n",
      "2. **Light2 (Dimming Light above TV):** This light is likely not necessary for reading purposes on the sofa, as it's typically used more for ambient lighting or watching TV.\n",
      "\n",
      "3. **Light3 (Lamp on Table):** Considering Richard is reading, this would be the most appropriate light to turn on and adjust to a comfortable brightness level suitable for reading. If the room is very bright due to external lights, reducing the brightness of Light1 might make it unnecessary, but keeping Light3 on will ensure good visibility.\n",
      "\n",
      "4. **Light4 (Curtain):** Since its still quite bright at 11:04 PM and Richard is reading indoors, opening or closing the curtains won't have a significant impact if there's no direct sunlight affecting the brightness inside the room. However, to reduce glare from external lights, closing Light4 might be beneficial.\n",
      "\n",
      "5. **Light5 (Main Light in Dining Room):** Since this light isnt directly related to Richards current activity of reading on the sofa and considering it is already very bright, theres no need to turn this light on or off unless it impacts the brightness level where he's sitting.\n",
      "\n",
      "Based on these considerations:\n",
      "\n",
      "- Turn off Light1.\n",
      "- Keep Light3 (adjust its dimmer if necessary).\n",
      "- Consider closing Light4 (if glare is an issue).\n",
      "\n",
      "**Action: Adjust based on reasoning.**\n",
      "\n",
      "**Final Light Control Actions:**\n",
      "```json\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04 PM when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "User Feedback:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}\n",
      "Assistant's Response:\n",
      "Given that Richard has been reading on the sofa multiple times between 10:46 PM and 11:04 PM when it is very bright, and he manually turned off all lights except for keeping light4 (the curtain) open, we should respect his current preference while ensuring a comfortable lighting environment.\n",
      "\n",
      "**Reasoning Steps:**\n",
      "\n",
      "- Since the room is already very bright at these times, turning off all lights might be too dark and could strain Richard's eyes. However, turning on any additional artificial light could cause glare and disrupt reading comfort.\n",
      "  \n",
      "- Light4 (curtain) was kept open by Richard manually, indicating he likely wants to maintain natural brightness or enjoy the view without closing it.\n",
      "\n",
      "Given these observations:\n",
      "\n",
      "**Action:**\n",
      "Maintain current settings but ensure that if there is still too much glare from external sources, a slight adjustment could be necessary for better comfort. However, as per recent preferences and behaviors, the simplest solution would be to keep Richards setup unchanged unless direct sunlight or excessive brightness causes discomfort.\n",
      "\n",
      "Considering these points:\n",
      "\n",
      "**Final Light Control Actions:**\n",
      "```json\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"off\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Updated Action:\n",
      "Given that Richard has been reading on the sofa multiple times between 10:46 PM and 11:04 PM when it is very bright, and he manually turned off all lights except for keeping light4 (the curtain) open, we should respect his current preference while ensuring a comfortable lighting environment.\n",
      "\n",
      "**Reasoning Steps:**\n",
      "\n",
      "- Since the room is already very bright at these times, turning off all lights might be too dark and could strain Richard's eyes. However, turning on any additional artificial light could cause glare and disrupt reading comfort.\n",
      "  \n",
      "- Light4 (curtain) was kept open by Richard manually, indicating he likely wants to maintain natural brightness or enjoy the view without closing it.\n",
      "\n",
      "Given these observations:\n",
      "\n",
      "**Action:**\n",
      "Maintain current settings but ensure that if there is still too much glare from external sources, a slight adjustment could be necessary for better comfort. However, as per recent preferences and behaviors, the simplest solution would be to keep Richards setup unchanged unless direct sunlight or excessive brightness causes discomfort.\n",
      "\n",
      "Considering these points:\n",
      "\n",
      "**Final Light Control Actions:**\n",
      "```json\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"reading on the sofa at 11:04:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"off\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "description not changed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mmain_react\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 133\u001b[0m, in \u001b[0;36mmain_react\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription not changed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def react(user, df_latest, user_feedback, df_history, actions):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_query = f\"\"\"\n",
    "\n",
    "    {user} is the user who is living in this apartment.\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, decide if to update the lights base based on the user feedback.\n",
    "\n",
    "    \n",
    "    Determine which lights should be turned on/off while the user is **{df_latest}**, considering the user preferences, lighting setup, user feedback, and recent user behaviors.\n",
    "    For each step, first provide your reasoning, then specify any action you would take (e.g., \"Action: [describe action]\").\n",
    "    If you need to recall user preferences or recent behaviors, indicate that you are accessing that information.\n",
    "    After reasoning and actions, provide the final light control actions in the fixed format.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "def main_react():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "    user = \"Richard\"\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = react(\n",
    "                user = user,\n",
    "                df_latest = df_latest,\n",
    "                user_feedback = None,\n",
    "                df_history = None,\n",
    "                actions = None)\n",
    "            \n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "        \n",
    "\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            actions_feedback = react(\n",
    "                user=user,\n",
    "                df_latest=df_latest,\n",
    "                user_feedback=user_feedback,\n",
    "                df_history=df_history,\n",
    "                actions=actions)\n",
    "            \n",
    "            print(\"\\nUpdated Action:\")\n",
    "            print(actions_feedback)\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_react()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# RAG\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def rag(user, knowledge_base, df_latest, collection, df_history, actions, user_feedback):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "\n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment. Based on the Layout of lights in the living room, \n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Decide if to update the knowledge base based on the user feedback and recent behaviors.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"User Query:\")\n",
    "    print(user_query)\n",
    "\n",
    "    query_response = ollama.embeddings(\n",
    "        prompt=user_query,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_response[\"embedding\"]],\n",
    "        n_results=10  # Retrieve top 3 relevant documents\n",
    "    )\n",
    "\n",
    "    retrieved_documents = results['documents'][0]  \n",
    "    data = ' '.join(retrieved_documents)\n",
    "    print(\"Retrieved Data:\")\n",
    "    print(data)\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Based on the knowledge base entries:\n",
    "    {data}\n",
    "\n",
    "    Respond to the user's current situation, user feedback and recent user behaviors:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "\n",
    "    Decide if to update the knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"Update\": \"yes/no\"  \n",
    "    }}\n",
    "\n",
    "    Provide the updated knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "def main_rag():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "\n",
    "    actions = None\n",
    "    user_feedback = None\n",
    "\n",
    "    user = \"Richard\"\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = rag(\n",
    "                user = user,\n",
    "                knowledge_base = knowledge_base,\n",
    "                df_latest=df_latest,\n",
    "                collection=collection,\n",
    "                df_history=df_history,\n",
    "                actions=actions,\n",
    "                user_feedback=user_feedback)\n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"on\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            if user_feedback:\n",
    "                user_condidence = get_user_confidence()\n",
    "\n",
    "                print(user_feedback)\n",
    "                # knowledge_base_update = LLM1(user_feedback, actions, df_history, collection)\n",
    "                knowledge_base_update = rag(\n",
    "                    user=user,\n",
    "                    df_latest=df_latest,\n",
    "                    user_feedback=user_feedback,\n",
    "                    actions=actions,\n",
    "                    df_history=df_history)\n",
    "                \n",
    "                knowledge_base = update_knowledge_base(knowledge_base, knowledge_base_update)\n",
    "                print(\"\\nUpdated Knowledge Base:\")\n",
    "                print(knowledge_base)\n",
    "\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_rag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csi",
   "language": "python",
   "name": "csi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
