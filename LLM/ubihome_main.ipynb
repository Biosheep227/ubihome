{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import ollama\n",
    "import chromadb\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# Pipeline functions\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    def interpret_brightness(brightness_level):\n",
    "        if 0 <= brightness_level <= 100:\n",
    "            return \"dark\"\n",
    "        elif 100 < brightness_level <= 150:\n",
    "            return \"dim\"\n",
    "        elif 150 < brightness_level <= 500:\n",
    "            return \"normal lighting\"\n",
    "        elif 500 < brightness_level <= 1000:\n",
    "            return \"very bright\"\n",
    "        else:\n",
    "            return \"dark\"\n",
    "\n",
    "    def generate_description(row):\n",
    "        brightness_description = interpret_brightness(row['BrightnessLevel'])\n",
    "        # return f\"{row['UserActivity']} in the {row['Location']} when it is {brightness_description} at {row['Timestamp']}.\"\n",
    "        pd.set_option('display.max_colwidth', None)\n",
    "        description = f\"{row['UserActivity']} in the {row['Location']} at {row['Timestamp']} when it is {brightness_description}.\"\n",
    "        return description\n",
    "\n",
    "    df['Description'] = df.apply(generate_description, axis=1)\n",
    "\n",
    "    # only use the lastest state\n",
    "    df_latest = df.iloc[-1:]\n",
    "    df_latest = df_latest['Description']\n",
    "    df_latest = df_latest.to_string(index=False)\n",
    "    # print(df_latest)\n",
    "\n",
    "    # use last 10 states\n",
    "    df_history = df.iloc[-10:]\n",
    "    df_history = df_history['Description']\n",
    "    df_history = df_history.to_string(index=False)\n",
    "    # print(df_history)\n",
    "\n",
    "    return df_latest, df_history\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    \"\"\"\n",
    "    Extracts the JSON object from the assistant's response.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    brace_count = 0\n",
    "    json_start = None\n",
    "\n",
    "    for i, char in enumerate(response_text):\n",
    "        if char == '{':\n",
    "            if brace_count == 0:\n",
    "                json_start = i\n",
    "            brace_count += 1\n",
    "        elif char == '}':\n",
    "            brace_count -= 1\n",
    "            if brace_count == 0 and json_start is not None:\n",
    "                json_str = response_text[json_start:i+1]\n",
    "                try:\n",
    "                    # Try to parse the extracted string\n",
    "                    json.loads(json_str)\n",
    "                    return json_str\n",
    "                except json.JSONDecodeError:\n",
    "                    pass  # Continue searching for valid JSON\n",
    "    raise ValueError(\"No valid JSON object found in the assistant's response.\")\n",
    "\n",
    "\n",
    "    \n",
    "def LLM2(knowledge_base, df_latest, collection, df_history, light_status):\n",
    "\n",
    "    # user_query = f\"\"\"\n",
    "    # There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    # Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    # \"\"\" \n",
    "\n",
    "    user_query = f\"\"\"\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. light5 is the main light in the dining room next to living room closely.\n",
    "    Now the light status is {light_status}, recent user behaviors are: {df_history}tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    \"\"\" \n",
    "\n",
    "    print(\"User Query:\")\n",
    "    print(user_query)\n",
    "\n",
    "    query_response = ollama.embeddings(\n",
    "        prompt=user_query,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_response[\"embedding\"]],\n",
    "        n_results= 10  # Retrieve top 10 relevant documents\n",
    "    )\n",
    "\n",
    "    retrieved_documents = results['documents'][0]  \n",
    "    data = ' '.join(retrieved_documents)\n",
    "    print(\"Retrieved Data:\")\n",
    "    print(data)\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Based on the knowledge base entries:\n",
    "    {data}\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"light1\": \"on/off\",\n",
    "        \"light2\": \"on/off\",\n",
    "        \"light3\": \"on/off\",\n",
    "        \"light4\": \"open/close\",\n",
    "        \"light5\": \"on/off\",\n",
    "\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    assistant_response = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(assistant_response)\n",
    "\n",
    "    try:\n",
    "        # Extract JSON code block if the assistant included any text before or after\n",
    "        json_code = extract_json_from_response(assistant_response)\n",
    "\n",
    "        # Parse the JSON string\n",
    "        actions = json.loads(json_code)\n",
    "\n",
    "        # Remove the \"description\" key if not needed\n",
    "        actions.pop(\"description\", None)\n",
    "\n",
    "        print(\"\\nExtracted Light Control Actions:\")\n",
    "        print(actions)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        # Handle the error or return None\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def LLM1(df_latest, user_feedback, actions, df_history, collection):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_update = f\"\"\"\n",
    "    You are a smart home assistant responsible for deciding if to update the lighting knowledge based on user manually changes to the light states. \n",
    "    The user manually change the current light status from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, there are some trasition phase, so decide if to update the knowledge base based on the user feedback and recent behaviors.\n",
    "    \"\"\"\n",
    "\n",
    "    query_response = ollama.embeddings(\n",
    "        prompt=user_update,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_response[\"embedding\"]],\n",
    "        n_results=8  # Retrieve top 10 relevant documents\n",
    "    )\n",
    "\n",
    "    retrieved_documents = results['documents'][0]  \n",
    "    data = ' '.join(retrieved_documents)\n",
    "    print(\"Retrieved Data:\")\n",
    "    print(data)\n",
    "\n",
    "    final_prompt_update = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Based on the retrieved current knowledge base entries:\n",
    "    {data}\n",
    "\n",
    "    Consider current situation:\n",
    "    {user_update}\n",
    "\n",
    "    Decide if to update the knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"Update\": \"yes/no\"  \n",
    "    }}\n",
    "\n",
    "    Provide the updated knowledge base in the fixed json format:\n",
    "    {{\n",
    "        \"{df_latest}:\",\n",
    "        \"light1\": \"on/off\",\n",
    "        \"light2\": \"on/off\",\n",
    "        \"light3\": \"on/off\",\n",
    "        \"light4\": \"open/close\",\n",
    "        \"light5\": \"on/off\",\n",
    "\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    ## LLM1\n",
    "    output = ollama.generate(\n",
    "    model=\"qwen2.5:14b-instruct\",\n",
    "    prompt=final_prompt_update\n",
    "    )\n",
    "\n",
    "    knowledge_base_text = output['response']\n",
    "    # print(knowledge_base_text)\n",
    "    return knowledge_base_text\n",
    "\n",
    "\n",
    "def extract_json_objects(text):\n",
    "    \"\"\"\n",
    "    Extracts all JSON objects from the text.\n",
    "    \"\"\"\n",
    "    json_objects = []\n",
    "    brace_stack = []\n",
    "    in_json = False\n",
    "    json_start = 0\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '{':\n",
    "            if not in_json:\n",
    "                json_start = i\n",
    "                in_json = True\n",
    "            brace_stack.append('{')\n",
    "        elif char == '}':\n",
    "            if brace_stack:\n",
    "                brace_stack.pop()\n",
    "                if not brace_stack:\n",
    "                    # Found a complete JSON object\n",
    "                    json_str = text[json_start:i+1]\n",
    "                    json_objects.append(json_str)\n",
    "                    in_json = False\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def update_knowledge_base(knowledge_base, knowledge_base_update):\n",
    "    code_block_pattern = r'```json\\s*([\\s\\S]*?)```'\n",
    "    json_blocks = re.findall(code_block_pattern, knowledge_base_update)\n",
    "\n",
    "    if not json_blocks:\n",
    "        # Try to find any JSON objects in the text\n",
    "        # json_blocks = re.findall(r'\\{(?:[^{}]|(?R))*\\}', knowledge_base_update)\n",
    "        json_blocks = extract_json_objects(knowledge_base_update)\n",
    "\n",
    "    update_decision = None\n",
    "    updated_kb = None\n",
    "\n",
    "    for idx, json_str in enumerate(json_blocks):\n",
    "        print(f\"\\nExtracted JSON String {idx+1}:\")\n",
    "        print(json_str)\n",
    "\n",
    "        # Clean up the JSON string\n",
    "        json_str = json_str.strip()\n",
    "        json_str = re.sub(r',\\s*}', '}', json_str)  # Remove trailing commas before }\n",
    "        json_str = re.sub(r',\\s*\\]', ']', json_str)  # Remove trailing commas before ]\n",
    "\n",
    "        # Parse the JSON string\n",
    "        try:\n",
    "            json_data = json.loads(json_str)\n",
    "\n",
    "            if \"Update\" in json_data:\n",
    "                update_decision = json_data.get(\"Update\")\n",
    "                print(f\"\\nParsed Update Decision: {update_decision}\")\n",
    "\n",
    "                json_data.pop(\"Update\", None)\n",
    "\n",
    "                if json_data:\n",
    "                    updated_kb = json_data\n",
    "                    print(\"\\nParsed Updated Knowledge Base:\")\n",
    "                    print(json.dumps(updated_kb, indent=2))\n",
    "            else:\n",
    "                updated_kb = json_data\n",
    "                print(\"\\nParsed Updated Knowledge Base:\")\n",
    "                print(json.dumps(updated_kb, indent=2))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"\\nFailed to parse JSON in block {idx+1}: {e}\")\n",
    "\n",
    "    # Process the extracted data\n",
    "    if update_decision is not None:\n",
    "        print(f\"\\nUpdate Decision: {update_decision}\")\n",
    "        if update_decision.lower() == \"yes\":\n",
    "            if updated_kb is not None:\n",
    "                # light_status = updated_kb.get(\"description\")\n",
    "                knowledge_base = knowledge_base + \"\\n\" + json.dumps(updated_kb, indent=2)\n",
    "                print(\"\\nKnowledge base has been updated.\")\n",
    "            else:\n",
    "                print(\"\\nNo updated knowledge base provided.\")\n",
    "        else:\n",
    "            print(\"\\nKnowledge base remains unchanged.\")\n",
    "    else:\n",
    "        print(\"\\nThe 'Update' decision was not found in the LLM's output.\")\n",
    "\n",
    "    return knowledge_base, update_decision\n",
    "\n",
    "\n",
    "def get_user_confidence():\n",
    "    confidence_input = input(\"Please rate your confidence in the system on a scale from 0 to 1: \")\n",
    "    try:\n",
    "        confidence = float(confidence_input)\n",
    "        if 0 <= confidence <= 1:\n",
    "            return confidence\n",
    "        else:\n",
    "            print(\"Invalid input. Confidence should be between 0 and 1.\")\n",
    "            return get_user_confidence()\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a number between 0 and 1.\")\n",
    "        return get_user_confidence()\n",
    "\n",
    "\n",
    "knowledge_base = \"\"\"\n",
    "    This is the preference of the user:\n",
    "    Prefer a dimly lit while watching TV.\n",
    "    Prefer a dimly lit environment while reading books.\n",
    "    Prefer a bright environment while exercising.\n",
    "    Prefer a bright environment while eating.\n",
    "    Prefer a dimly lit environment while playing video games.\n",
    "    Prefer a dark environment while lying. \n",
    "    Prefer a bright environment while exercising.\n",
    "    Prefer a bright environment while housekeeping.\n",
    "    Prefer a dimly lit environment while typing.\n",
    "    Prefer a dimly lit environment while sitting.\n",
    "    Prefer a bright environment while cooking.\n",
    "    Prefer a bright environment while grooming.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_json = \"\"\"\n",
    "{\n",
    "  \"Layout of lights in the living room\": [\n",
    "    {\n",
    "      \"light_id\": \"light1\",\n",
    "      \"description\": \"Main light with 3 color temperatures: warm, neutral, and cool.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light2\",\n",
    "      \"description\": \"Dimming light above the TV.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light3\",\n",
    "      \"description\": \"Lamp on the table.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light4\",\n",
    "      \"description\": \"Curtain on the window.\"\n",
    "    },\n",
    "    {\n",
    "      \"light_id\": \"light5\",\n",
    "      \"description\": \"Main light in the dining room next to the living room.\"\n",
    "    }\n",
    "    \n",
    "  ],\n",
    "  \"users\": [\n",
    "    {\n",
    "      \"user_id\": \"user1\",\n",
    "      \"name\": \"Richard\",\n",
    "      \"preferences\": [\n",
    "        {\n",
    "          \"activity\": \"watching TV\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"reading\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"cooking\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"sitting\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"lying\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"typing\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"housekeeping\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"exercising\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"video gaming\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"eating\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"grooming\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user2\",\n",
    "      \"name\": \"Di\",\n",
    "      \"preferences\": [\n",
    "        {\n",
    "          \"activity\": \"watching TV\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"reading\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"cooking\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"sitting\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"lying\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"typing\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"housekeeping\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"exercising\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"video gaming\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"eating\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"grooming\",\n",
    "            \"preferred_lighting\": \"dimly lit\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"user_id\": \"user3\",\n",
    "      \"name\": \"Mingyi\",\n",
    "      \"preferences\": [\n",
    "        {\n",
    "          \"activity\": \"watching TV\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"reading\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"cooking\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"sitting\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"lying\",\n",
    "          \"preferred_lighting\": \"dark\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"typing\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"housekeeping\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"exercising\",\n",
    "          \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "          \"activity\": \"video gaming\",\n",
    "          \"preferred_lighting\": \"dimly lit\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"eating\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        },\n",
    "        {\n",
    "            \"activity\": \"grooming\",\n",
    "            \"preferred_lighting\": \"bright\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "  ]\n",
    "}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# Pipeline Main\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "housekeeping in the living room at 14:26:00 when it is very bright.\n",
      "User Query:\n",
      "\n",
      "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. light5 is the main light in the dining room next to living room closely.\n",
      "    Now the light status is {'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'off'}, recent user behaviors are:      walking in the living room at 14:08:00 when it is very bright.\n",
      "housekeeping in the living room at 14:10:00 when it is very bright.\n",
      "housekeeping in the living room at 14:12:00 when it is very bright.\n",
      "housekeeping in the living room at 14:14:00 when it is very bright.\n",
      "housekeeping in the living room at 14:16:00 when it is very bright.\n",
      "housekeeping in the living room at 14:18:00 when it is very bright.\n",
      "housekeeping in the living room at 14:20:00 when it is very bright.\n",
      "housekeeping in the living room at 14:22:00 when it is very bright.\n",
      "housekeeping in the living room at 14:24:00 when it is very bright.\n",
      "housekeeping in the living room at 14:26:00 when it is very bright.tell me Which lights should be turned on/off while the user is housekeeping in the living room at 14:26:00 when it is very bright.? \"\n",
      "    \n",
      "Retrieved Data:\n",
      "Prefer a bright environment while housekeeping. Prefer a bright environment while cooking. Prefer a bright environment while grooming. Prefer a bright environment while eating. Prefer a dimly lit environment while sitting. Prefer a dimly lit while watching TV. Prefer a dark environment while lying. Prefer a dimly lit environment while typing. Prefer a bright environment while exercising. Prefer a bright environment while exercising.\n",
      "Assistant's Response:\n",
      "{\n",
      "    \"description\": \"housekeeping in the living room at 14:26:00 when it is very bright.\",\n",
      "    \"light1\": \"off\",\n",
      "    \"light2\": \"off\",\n",
      "    \"light3\": \"off\",\n",
      "    \"light4\": \"close\",\n",
      "    \"light5\": \"off\"\n",
      "}\n",
      "\n",
      "Since the user is currently in a brightly lit environment and engaged in housekeeping, which prefers a bright environment according to your knowledge base entries, no changes are needed for lighting conditions. The current status of all lights being off along with curtains closed implies natural sunlight illuminating the living room sufficiently for housekeeping activities. Therefore, there's no need to adjust any light settings given that the user is already in a very bright environment, which aligns perfectly with their preference while housekeeping.\n",
      "\n",
      "Extracted Light Control Actions:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'off'}\n",
      "\n",
      "Suggested Action:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'off'}\n",
      "Invalid JSON input. Please try again.\n",
      "description changed\n",
      "housekeeping in the living room at 14:26:00 when it is very bright.\n",
      "User Query:\n",
      "\n",
      "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. light5 is the main light in the dining room next to living room closely.\n",
      "    Now the light status is {'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'off'}, recent user behaviors are:      walking in the living room at 14:08:00 when it is very bright.\n",
      "housekeeping in the living room at 14:10:00 when it is very bright.\n",
      "housekeeping in the living room at 14:12:00 when it is very bright.\n",
      "housekeeping in the living room at 14:14:00 when it is very bright.\n",
      "housekeeping in the living room at 14:16:00 when it is very bright.\n",
      "housekeeping in the living room at 14:18:00 when it is very bright.\n",
      "housekeeping in the living room at 14:20:00 when it is very bright.\n",
      "housekeeping in the living room at 14:22:00 when it is very bright.\n",
      "housekeeping in the living room at 14:24:00 when it is very bright.\n",
      "housekeeping in the living room at 14:26:00 when it is very bright.tell me Which lights should be turned on/off while the user is housekeeping in the living room at 14:26:00 when it is very bright.? \"\n",
      "    \n",
      "Retrieved Data:\n",
      "Prefer a bright environment while housekeeping. Prefer a bright environment while cooking. Prefer a bright environment while grooming. Prefer a bright environment while eating. Prefer a dimly lit environment while sitting. Prefer a dimly lit while watching TV. Prefer a dark environment while lying. Prefer a dimly lit environment while typing. Prefer a bright environment while exercising. Prefer a bright environment while exercising.\n",
      "Assistant's Response:\n",
      "{\n",
      "    \"description\": \"housekeeping in the living room at 14:26:00 when it is very bright.\",\n",
      "    \"light1\": \"off\",\n",
      "    \"light2\": \"off\",\n",
      "    \"light3\": \"off\",\n",
      "    \"light4\": \"close\",\n",
      "    \"light5\": \"off\"\n",
      "}\n",
      "\n",
      "Since the environment is already very bright, no additional lights need to be turned on for housekeeping activities. The curtains should remain closed as they are currently.\n",
      "\n",
      "Extracted Light Control Actions:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'off'}\n",
      "\n",
      "Suggested Action:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'off'}\n",
      "User Feedback:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'close', 'light5': 'on'}\n",
      "Retrieved Data:\n",
      "Prefer a bright environment while housekeeping. Prefer a bright environment while grooming. Prefer a bright environment while cooking. This is the preference of the user: Prefer a bright environment while eating. Prefer a bright environment while exercising. Prefer a bright environment while exercising. Prefer a dark environment while lying.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 76\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m user_condidence \u001b[38;5;241m=\u001b[39m get_user_confidence()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# if user_condidence < 0.5:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#     # use RL agent to update the knowledge base\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m#     print(\"RL agent to update the knowledge base\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# print(user_feedback)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# knowledge_base_update = LLM1(user_feedback, actions, df_history, collection)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m knowledge_base_update \u001b[38;5;241m=\u001b[39m \u001b[43mLLM1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_latest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_latest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_feedback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_feedback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m knowledge_base, update_decision \u001b[38;5;241m=\u001b[39m update_knowledge_base(knowledge_base, knowledge_base_update)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUpdated Knowledge Base:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[33], line 204\u001b[0m, in \u001b[0;36mLLM1\u001b[0;34m(df_latest, user_feedback, actions, df_history, collection)\u001b[0m\n\u001b[1;32m    177\u001b[0m final_prompt_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124mYou are a smart home assistant.\u001b[39m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m## LLM1\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqwen2.5:14b-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_prompt_update\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m knowledge_base_text \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# print(knowledge_base_text)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/ollama/_client.py:163\u001b[0m, in \u001b[0;36mClient.generate\u001b[0;34m(self, model, prompt, suffix, system, template, context, stream, raw, format, images, options, keep_alive)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[1;32m    161\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust provide a model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuffix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemplate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_encode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/ollama/_client.py:99\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[0;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[1;32m     94\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     95\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m     96\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     98\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m---> 99\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/ollama/_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http_proxy.py:207\u001b[0m, in \u001b[0;36mForwardHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    194\u001b[0m url \u001b[38;5;241m=\u001b[39m URL(\n\u001b[1;32m    195\u001b[0m     scheme\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_origin\u001b[38;5;241m.\u001b[39mscheme,\n\u001b[1;32m    196\u001b[0m     host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_origin\u001b[38;5;241m.\u001b[39mhost,\n\u001b[1;32m    197\u001b[0m     port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_proxy_origin\u001b[38;5;241m.\u001b[39mport,\n\u001b[1;32m    198\u001b[0m     target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbytes\u001b[39m(request\u001b[38;5;241m.\u001b[39murl),\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m proxy_request \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    201\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    202\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    206\u001b[0m )\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/csi/lib/python3.9/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "documents = knowledge_base.split('\\n\\n')\n",
    "documents = [' '.join(doc.split()) for doc in documents]\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "try:\n",
    "    collection = client.get_collection(\"docs\")\n",
    "    print(\"Collection already exists.\")\n",
    "except Exception as e:\n",
    "    # If the collection does not exist, create it\n",
    "    collection = client.create_collection(name=\"docs\")\n",
    "    print(\"Collection created.\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "    response = ollama.embeddings(model=\"mxbai-embed-large\", prompt=d)\n",
    "    embedding = response[\"embedding\"]\n",
    "    collection.add(\n",
    "        ids=[str(i)],\n",
    "        embeddings=[embedding],\n",
    "        documents=[d]\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "\n",
    "    user = \"Richard\"\n",
    "    light_status = {\n",
    "        \"light1\": \"off\",\n",
    "        \"light2\": \"off\",\n",
    "        \"light3\": \"off\",\n",
    "        \"light4\": \"close\",\n",
    "        \"light5\": \"off\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data/reading.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = LLM2(knowledge_base, df_latest, collection, df_history, light_status)\n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"close\", \"light5\": \"off\"}\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    # user_feedback = None\n",
    "                    continue\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            if user_feedback:\n",
    "                user_condidence = get_user_confidence()\n",
    "                # if user_condidence < 0.5:\n",
    "                #     # use RL agent to update the knowledge base\n",
    "                #     print(\"RL agent to update the knowledge base\")\n",
    "                #     knowledge_base = rl_agent_update(knowledge_base, user_feedback, actions, df_history)\n",
    "\n",
    "                # else:\n",
    "                #     knowledge_base_update = LLM1(user_feedback, actions, df_history)\n",
    "                #     knowledge_base = update_knowledge_base(knowledge_base, knowledge_base_update)\n",
    "                #     print(\"\\nUpdated Knowledge Base:\")\n",
    "                #     print(knowledge_base)\n",
    "                # print(user_feedback)\n",
    "                # knowledge_base_update = LLM1(user_feedback, actions, df_history, collection)\n",
    "                knowledge_base_update = LLM1(\n",
    "                    df_latest=df_latest,\n",
    "                    user_feedback=user_feedback,\n",
    "                    actions=actions,\n",
    "                    df_history=df_history,\n",
    "                    collection=collection)\n",
    "                \n",
    "                knowledge_base, update_decision = update_knowledge_base(knowledge_base, knowledge_base_update)\n",
    "                print(\"\\nUpdated Knowledge Base:\")\n",
    "                print(knowledge_base)\n",
    "\n",
    "                if update_decision == \"yes\":\n",
    "                    light_status = user_feedback\n",
    "                    print(\"\\nUpdated Light Status:\")\n",
    "                    print(light_status)\n",
    "                else:\n",
    "                    print(\"\\nLight Status remains unchanged.\")\n",
    "                    light_status = actions\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This is the preference of the user:\n",
      "    Prefer a dimly lit while watching TV.\n",
      "    Prefer a dimly lit environment while reading books.\n",
      "    Prefer a bright environment while exercising.\n",
      "    Prefer a bright environment while eating.\n",
      "    Prefer a dimly lit environment while playing video games.\n",
      "    Prefer a dark environment while lying. \n",
      "    Prefer a bright environment while exercising.\n",
      "    Prefer a bright environment while housekeeping.\n",
      "    Prefer a dimly lit environment while typing.\n",
      "    Prefer a dimly lit environment while sitting.\n",
      "    Prefer a bright environment while cooking.\n",
      "    Prefer a bright environment while grooming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# 0-Shot\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "housekeeping in the living_room at 14:15:00 when it is very bright.\n",
      "User Feedback:\n",
      "None\n",
      "Assistant's Response:\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Extracted Light Control Actions:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light1': 'off', 'light2': 'off', 'light3': 'on', 'light4': 'close', 'light5': 'off'}}\n",
      "\n",
      "Suggested Action:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light1': 'off', 'light2': 'off', 'light3': 'on', 'light4': 'close', 'light5': 'off'}}\n",
      "User Feedback:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}\n",
      "Assistant's Response:\n",
      "Based on Richard's recent behaviors and the current brightness, it seems that he prefers having no lights turned on during housekeeping activities in a very bright environment. Here is the updated light status based on his feedback:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light3\": \"off\",\n",
      "        \"light4\": \"open\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Since Richard manually turned off light3 and opened the curtains (light4), I'm suggesting keeping those settings as they reflect his preference for working in a well-lit room without artificial lighting. The other lights remain off as previously suggested, since there is already plenty of natural light during housekeeping activities.\n",
      "\n",
      "Extracted Light Control Actions:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light3': 'off', 'light4': 'open'}}\n",
      "\n",
      "Updated Action:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light3': 'off', 'light4': 'open'}}\n",
      "description not changed\n",
      "description not changed\n",
      "description not changed\n",
      "description not changed\n",
      "description not changed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mmain_zeroshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 159\u001b[0m, in \u001b[0;36mmain_zeroshot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription not changed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def zero_shot(user, df_latest, user_feedback, df_history, actions, light_status):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment. \n",
    "\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Now the current light status is {light_status}.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, decide if to update the light status base based on the user feedback.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"light1\": \"on/off\",\n",
    "        \"light2\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    assistant_response = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(assistant_response)\n",
    "\n",
    "    try:\n",
    "        # Extract JSON code block if the assistant included any text before or after\n",
    "        json_code = extract_json_from_response(assistant_response)\n",
    "\n",
    "        # Parse the JSON string\n",
    "        actions = json.loads(json_code)\n",
    "\n",
    "        # Remove the \"description\" key if not needed\n",
    "        actions.pop(\"description\", None)\n",
    "\n",
    "        print(\"\\nExtracted Light Control Actions:\")\n",
    "        print(actions)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        # Handle the error or return None\n",
    "        return None\n",
    "    \n",
    "\n",
    "def main_zeroshot():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "    user = \"Richard\"\n",
    "    light_status = {\n",
    "        \"light1\": \"off\",\n",
    "        \"light2\": \"off\",\n",
    "        \"light3\": \"off\",\n",
    "        \"light4\": \"close\",\n",
    "        \"light5\": \"off\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data/reading.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = zero_shot(\n",
    "                user = user,\n",
    "                df_latest = df_latest,\n",
    "                user_feedback = None,\n",
    "                df_history = None,\n",
    "                actions = None,\n",
    "                light_status = light_status)\n",
    "            \n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "        \n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            light_status = actions\n",
    "\n",
    "            actions_feedback = zero_shot(\n",
    "                user=user,\n",
    "                df_latest=df_latest,\n",
    "                user_feedback=user_feedback,\n",
    "                df_history=df_history,\n",
    "                actions=actions,\n",
    "                light_status=actions)\n",
    "            \n",
    "            print(\"\\nUpdated Action:\")\n",
    "            print(actions_feedback)\n",
    "\n",
    "            light_status = actions_feedback\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_zeroshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# Chain of Thought\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "housekeeping in the living_room at 14:15:00 when it is very bright.\n",
      "User Feedback:\n",
      "None\n",
      "Assistant's Response:\n",
      "Since Richard is housekeeping during a time when it is very bright outside, the need for artificial lighting might be minimal. However, to ensure that he can see clearly and comfortably while cleaning, we should consider turning on lights that provide subtle illumination without being too harsh.\n",
      "\n",
      "Given this context, here's an appropriate suggestion:\n",
      "\n",
      "- light1: This main light with three color temperatures could be set to a neutral or slightly cool setting at low brightness.\n",
      "- light3: The lamp on the table might provide focused lighting where Richard is working.\n",
      "- Light2 and light4 are less relevant for housekeeping purposes in bright daylight conditions. Keeping them off will minimize distractions.\n",
      "\n",
      "Based on these considerations, here's how we could adjust:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"on\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "This setup balances visibility and comfort without overwhelming natural light.\n",
      "\n",
      "Extracted Light Control Actions:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light1': 'on', 'light2': 'off', 'light3': 'on', 'light4': 'close', 'light5': 'off'}}\n",
      "\n",
      "Suggested Action:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light1': 'on', 'light2': 'off', 'light3': 'on', 'light4': 'close', 'light5': 'off'}}\n",
      "User Feedback:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}\n",
      "Assistant's Response:\n",
      "Given Richard's recent behaviors and preferences, it appears that he prefers to have the lights off during housekeeping activities when it is very bright outside. This pattern has been consistent for at least 10 instances before his last action.\n",
      "\n",
      "Since Richard manually turned all lights off (light1, light2, light3) while keeping the curtain open and leaving light5 in the dining room off, this indicates that he finds no need for additional lighting during these activities under bright conditions. Therefore, it would be reasonable to adhere to his preferences by maintaining the current state.\n",
      "\n",
      "Here is the suggested action based on Richard's recent behavior:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"off\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Extracted Light Control Actions:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}}\n",
      "\n",
      "Updated Action:\n",
      "{'1.': {'description': 'housekeeping in the living_room at 14:15:00 when it is very bright.', 'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}}\n",
      "description not changed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 162\u001b[0m\n\u001b[1;32m    159\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mmain_cot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 159\u001b[0m, in \u001b[0;36mmain_cot\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription not changed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def cot(user, df_latest, user_feedback, df_history, actions, light_status):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment.\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Now the current light status is {light_status}.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, decide if to update the lights base based on the user feedback.\n",
    "\n",
    "    Provide your reasoning step by step.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"light1\": \"on/off\",\n",
    "        \"light2\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    assistant_response = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(assistant_response)\n",
    "\n",
    "    try:\n",
    "        # Extract JSON code block if the assistant included any text before or after\n",
    "        json_code = extract_json_from_response(assistant_response)\n",
    "\n",
    "        # Parse the JSON string\n",
    "        actions = json.loads(json_code)\n",
    "\n",
    "        # Remove the \"description\" key if not needed\n",
    "        actions.pop(\"description\", None)\n",
    "\n",
    "        print(\"\\nExtracted Light Control Actions:\")\n",
    "        print(actions)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        # Handle the error or return None\n",
    "        return None\n",
    "    \n",
    "\n",
    "def main_cot():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "    user =  \"Richard\"\n",
    "    light_status = {\n",
    "        \"light1\": \"off\",\n",
    "        \"light2\": \"off\",\n",
    "        \"light3\": \"off\",\n",
    "        \"light4\": \"close\",\n",
    "        \"light5\": \"off\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data/reading.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = cot(\n",
    "                user = user,\n",
    "                df_latest = df_latest,\n",
    "                user_feedback = None,\n",
    "                df_history = None,\n",
    "                actions = None,\n",
    "                light_status = light_status)\n",
    "            \n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "        \n",
    "\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            actions_feedback = cot(\n",
    "                user=user,\n",
    "                df_latest=df_latest,\n",
    "                user_feedback=user_feedback,\n",
    "                df_history=df_history,\n",
    "                actions=actions,\n",
    "                light_status=actions)\n",
    "            \n",
    "            print(\"\\nUpdated Action:\")\n",
    "            print(actions_feedback)\n",
    "\n",
    "            light_status = actions_feedback\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_cot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# ReACT\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description changed\n",
      "housekeeping in the living_room at 14:15:00 when it is very bright.\n",
      "User Feedback:\n",
      "None\n",
      "Assistant's Response:\n",
      "Given that Richard is housekeeping in the living room at 14:15 and it is very bright outside, he might not need much artificial light. However, turning on a lamp can provide localized lighting which may be useful for detailed tasks like dusting or cleaning small items.\n",
      "\n",
      "- **light3 (lamp on table)**: Since it's very bright outside, overall room brightness isn't an issue, but having a lamp turned on could help with visibility during housekeeping activities.\n",
      "  \n",
      "There is no need to adjust the main light and TV dimming light since natural light provides sufficient illumination for general cleaning. The curtains are closed which might be beneficial if direct sunlight is causing glare or making it difficult to see.\n",
      "\n",
      "Since there's no user feedback or recent behavior to consider, let's proceed with turning on just one lamp for additional localized lighting during housekeeping activities.\n",
      "\n",
      "Action: Turn on the table lamp (light3).\n",
      "\n",
      "Final light control actions:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Suggested Action:\n",
      "Given that Richard is housekeeping in the living room at 14:15 and it is very bright outside, he might not need much artificial light. However, turning on a lamp can provide localized lighting which may be useful for detailed tasks like dusting or cleaning small items.\n",
      "\n",
      "- **light3 (lamp on table)**: Since it's very bright outside, overall room brightness isn't an issue, but having a lamp turned on could help with visibility during housekeeping activities.\n",
      "  \n",
      "There is no need to adjust the main light and TV dimming light since natural light provides sufficient illumination for general cleaning. The curtains are closed which might be beneficial if direct sunlight is causing glare or making it difficult to see.\n",
      "\n",
      "Since there's no user feedback or recent behavior to consider, let's proceed with turning on just one lamp for additional localized lighting during housekeeping activities.\n",
      "\n",
      "Action: Turn on the table lamp (light3).\n",
      "\n",
      "Final light control actions:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"on\",\n",
      "        \"light4\": \"close\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "User Feedback:\n",
      "{'light1': 'off', 'light2': 'off', 'light3': 'off', 'light4': 'open', 'light5': 'off'}\n",
      "Assistant's Response:\n",
      "Given that Richard has consistently been housekeeping during these times and manually turned off the table lamp (light3) while keeping all other lights off, it seems he prefers working without additional artificial lighting in a naturally bright room. Additionally, opening the curtains aligns with his previous actions.\n",
      "\n",
      "Considering this pattern:\n",
      "\n",
      "- **Light1** (main light): No need to turn on as natural light is sufficient.\n",
      "- **Light2** (TV dimming light): Not necessary for housekeeping activities.\n",
      "- **Light3** (table lamp): Richard manually turned off, indicating he prefers no artificial lighting during these tasks.\n",
      "- **Light4** (curtain): Opened by Richard, likely to avoid glare but still allow natural light.\n",
      "- **Light5** (dining room main light): No need to turn on since it’s not relevant to the living room activity.\n",
      "\n",
      "Action: Keep all lights as per user's manual adjustment, i.e., no artificial lighting while curtains are open for natural light.\n",
      "\n",
      "Final light control actions:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"off\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n",
      "\n",
      "Updated Action:\n",
      "Given that Richard has consistently been housekeeping during these times and manually turned off the table lamp (light3) while keeping all other lights off, it seems he prefers working without additional artificial lighting in a naturally bright room. Additionally, opening the curtains aligns with his previous actions.\n",
      "\n",
      "Considering this pattern:\n",
      "\n",
      "- **Light1** (main light): No need to turn on as natural light is sufficient.\n",
      "- **Light2** (TV dimming light): Not necessary for housekeeping activities.\n",
      "- **Light3** (table lamp): Richard manually turned off, indicating he prefers no artificial lighting during these tasks.\n",
      "- **Light4** (curtain): Opened by Richard, likely to avoid glare but still allow natural light.\n",
      "- **Light5** (dining room main light): No need to turn on since it’s not relevant to the living room activity.\n",
      "\n",
      "Action: Keep all lights as per user's manual adjustment, i.e., no artificial lighting while curtains are open for natural light.\n",
      "\n",
      "Final light control actions:\n",
      "\n",
      "{\n",
      "    \"1.\": {\n",
      "        \"description\": \"housekeeping in the living_room at 14:15:00 when it is very bright.\",\n",
      "        \"light1\": \"off\",\n",
      "        \"light2\": \"off\",\n",
      "        \"light3\": \"off\",\n",
      "        \"light4\": \"open\",\n",
      "        \"light5\": \"off\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mmain_react\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 145\u001b[0m, in \u001b[0;36mmain_react\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription not changed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def react(user, df_latest, user_feedback, df_history, actions, light_status):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "    \n",
    "    user_query = f\"\"\"\n",
    "\n",
    "    {user} is the user who is living in this apartment.\n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Now the current light status is {light_status}.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Considering recent user behaviors, decide if to update the lights base based on the user feedback.\n",
    "\n",
    "    \n",
    "    Determine which lights should be turned on/off while the user is **{df_latest}**, considering the user preferences, lighting setup, user feedback, and recent user behaviors.\n",
    "    For each step, first provide your reasoning, then specify any action you would take (e.g., \"Action: [describe action]\").\n",
    "    If you need to recall user preferences or recent behaviors, indicate that you are accessing that information.\n",
    "    After reasoning and actions, provide the final light control actions in the fixed format.\n",
    "    \"\"\"\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Respond to the user's current situation:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"light1\": \"on/off\",\n",
    "        \"light2\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "def main_react():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "    user = \"Richard\"\n",
    "    light_status = {\n",
    "        \"light1\": \"off\",\n",
    "        \"light2\": \"off\",\n",
    "        \"light3\": \"off\",\n",
    "        \"light4\": \"close\",\n",
    "        \"light5\": \"off\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data/reading.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = react(\n",
    "                user = user,\n",
    "                df_latest = df_latest,\n",
    "                user_feedback = None,\n",
    "                df_history = None,\n",
    "                actions = None,\n",
    "                light_status = light_status)\n",
    "            \n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"off\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "        \n",
    "\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            actions_feedback = react(\n",
    "                user=user,\n",
    "                df_latest=df_latest,\n",
    "                user_feedback=user_feedback,\n",
    "                df_history=df_history,\n",
    "                actions=actions,\n",
    "                light_status=actions)\n",
    "            \n",
    "            print(\"\\nUpdated Action:\")\n",
    "            print(actions_feedback)\n",
    "\n",
    "            light_status = actions_feedback\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_react()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================\n",
    "# RAG\n",
    "============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_query = '''\n",
    "    This is the preference of user1 Richard:\n",
    "\n",
    "    Richard likes a bright environment.\n",
    "\n",
    "    Richard requires a dimly lit while watching TV.\n",
    "\n",
    "    Richard requires a dimly lit environment while reading books.\n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while eating.\n",
    "\n",
    "    Richard requires a dimly lit environment while playing video games.\n",
    "\n",
    "    Richard requires a dark environment while sleeping (lying). \n",
    "\n",
    "    Richard requires a bright environment while exercising.\n",
    "\n",
    "    Richard requires a bright environment while housekeeping.\n",
    "\n",
    "    Richard requires a dimly lit environment while working (typing).\n",
    "\n",
    "    Richard requires a dimly lit environment while sitting.\n",
    "\n",
    "    Richard requires a bright environment while cooking.\n",
    "\n",
    "    Richard requires a bright environment while grooming.\n",
    "    '''\n",
    "\n",
    "def rag(user, knowledge_base, df_latest, collection, df_history, actions, user_feedback):\n",
    "    print(\"User Feedback:\")\n",
    "    print(user_feedback)\n",
    "\n",
    "    user_query = f\"\"\"\n",
    "    {user} is the user who is living in this apartment. Based on the Layout of lights in the living room, \n",
    "    There are 5 lights in the living room. light1 in the main light which has 3 color temperatures, warm, neutral and cool. light2 is a dimming light above the TV. light3 is a lamp on the table. light4 a curtain on the window. Dining room is next to living room, light5 is the main light in the dining room.\n",
    "    Tell me Which lights should be turned on/off while the user is {df_latest}? \"\n",
    "    Then based on the user feedback: {user_feedback} and recent user behaviors: {df_history}, decide which lights should be turned on/off.\n",
    "\n",
    "    The user manually changed the light state from suggested action: {actions} to: {user_feedback}. The recent 10 user behaviors are {df_history}.\n",
    "    Decide if to update the knowledge base based on the user feedback and recent behaviors.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"User Query:\")\n",
    "    print(user_query)\n",
    "\n",
    "    query_response = ollama.embeddings(\n",
    "        prompt=user_query,\n",
    "        model=\"mxbai-embed-large\"\n",
    "    )\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_response[\"embedding\"]],\n",
    "        n_results=10  # Retrieve top 3 relevant documents\n",
    "    )\n",
    "\n",
    "    retrieved_documents = results['documents'][0]  \n",
    "    data = ' '.join(retrieved_documents)\n",
    "    print(\"Retrieved Data:\")\n",
    "    print(data)\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "    You are a smart home assistant.\n",
    "\n",
    "    Based on the knowledge base entries:\n",
    "    {data}\n",
    "\n",
    "    Respond to the user's current situation, user feedback and recent user behaviors:\n",
    "    {user_query}\n",
    "\n",
    "    Only provide the light control actions in the fixed format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "\n",
    "    Decide if to update the knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"Update\": \"yes/no\"  \n",
    "    }}\n",
    "\n",
    "    Provide the updated knowledge base in the fixed json format:\n",
    "    {{\n",
    "    \"1.\": {{\n",
    "        \"description\": \"{df_latest}\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        \"lightx\": \"on/off\",\n",
    "        ...\n",
    "    }},\n",
    "    ...\n",
    "    }}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    output = ollama.generate(\n",
    "        # model=\"qwen2.5:7b\",\n",
    "        model =\"qwen2.5:14b-instruct\",\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "\n",
    "    actions = output['response']\n",
    "\n",
    "    print(\"Assistant's Response:\")\n",
    "    print(actions)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "def main_rag():\n",
    "    global knowledge_base\n",
    "    previous_df_latest = None\n",
    "\n",
    "    actions = None\n",
    "    user_feedback = None\n",
    "\n",
    "    user = \"Richard\"\n",
    "\n",
    "    while True:\n",
    "        df_latest, df_history = load_data('simulated_data.csv')\n",
    "\n",
    "        # check if df_latest changed\n",
    "        if df_latest != previous_df_latest:\n",
    "            print(\"description changed\")\n",
    "            print(df_latest)\n",
    "            \n",
    "            actions = rag(\n",
    "                user = user,\n",
    "                knowledge_base = knowledge_base,\n",
    "                df_latest=df_latest,\n",
    "                collection=collection,\n",
    "                df_history=df_history,\n",
    "                actions=actions,\n",
    "                user_feedback=user_feedback)\n",
    "            print(\"\\nSuggested Action:\")\n",
    "            print(actions)\n",
    "\n",
    "            user_feedback_input = input(\"Enter user feedback as a JSON string (or leave empty): \")\n",
    "            # format: {\"light1\": \"off\", \"light2\": \"on\", \"light3\": \"off\", \"light4\": \"open\", \"light5\": \"off\"}\n",
    "            if user_feedback_input:\n",
    "                try:\n",
    "                    user_feedback = json.loads(user_feedback_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON input. Please try again.\")\n",
    "                    user_feedback = None\n",
    "            else:\n",
    "                user_feedback = None\n",
    "\n",
    "            if user_feedback:\n",
    "                user_condidence = get_user_confidence()\n",
    "\n",
    "                print(user_feedback)\n",
    "                # knowledge_base_update = LLM1(user_feedback, actions, df_history, collection)\n",
    "                knowledge_base_update = rag(\n",
    "                    user=user,\n",
    "                    df_latest=df_latest,\n",
    "                    user_feedback=user_feedback,\n",
    "                    actions=actions,\n",
    "                    df_history=df_history)\n",
    "                \n",
    "                knowledge_base = update_knowledge_base(knowledge_base, knowledge_base_update)\n",
    "                print(\"\\nUpdated Knowledge Base:\")\n",
    "                print(knowledge_base)\n",
    "\n",
    "\n",
    "            previous_df_latest = df_latest\n",
    "        else:\n",
    "            print(\"description not changed\")\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_rag()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csi",
   "language": "python",
   "name": "csi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
